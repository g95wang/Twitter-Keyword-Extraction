{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install giphy_client Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: giphy_client in /home/haochen/.local/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from giphy_client) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from giphy_client) (1.14.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/lib/python3/dist-packages (from giphy_client) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from giphy_client) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install giphy_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Input Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import preprocessor as p\n",
    "import string\n",
    "import spacy\n",
    "import csv\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word, dictionary):\n",
    "    cur_max = 0\n",
    "    cur_word = ''\n",
    "    for w in dictionary:                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "        score = SequenceMatcher(None, w, word).ratio() * 100\n",
    "        if score > cur_max:\n",
    "            cur_max = score\n",
    "            cur_word = w\n",
    "    return cur_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dictionary():\n",
    "    file = open('data_preprocessing/raw_data/all_words.txt', 'r')\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    index = 0\n",
    "    dictionary = {}\n",
    "    for w in lines:\n",
    "        dictionary[w.strip('\\n')] = index\n",
    "        index += 1\n",
    "\n",
    "    file.close()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet, filename):\n",
    "    dictionary = setup_dictionary()\n",
    "    \n",
    "    # preprocessor\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    cleaned = p.clean(tweet)\n",
    "    if len(cleaned) > 0:\n",
    "        if cleaned[0] == ':':\n",
    "            cleaned = cleaned[1:]\n",
    "    \n",
    "    dependency_tagged = nlp(cleaned)\n",
    "    \n",
    "    with open(filename, \"a\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for token in dependency_tagged:\n",
    "\n",
    "            if token.lemma_ != '-PRON-' and token.pos_ != 'SPACE' and token.text not in string.punctuation and token.text.isdigit() == False and token.pos_ != 'PUNCT' and token.pos_ != 'NUM' and token.pos_ != 'X':\n",
    "                lower_case = token.lemma_.lower()\n",
    "                word_index = 0\n",
    "\n",
    "                if lower_case in dictionary:\n",
    "                    word_index = dictionary[lower_case]\n",
    "\n",
    "                else:\n",
    "                    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_stop)\n",
    "                    lower_case = find_similar(lower_case, dictionary)\n",
    "                    if lower_case == '':\n",
    "#                         print('############# unfound ############')\n",
    "                        continue\n",
    "                    word_index = dictionary[lower_case]\n",
    "#                     print(lower_case, word_index)\n",
    "\n",
    "#                 if lower_case not in twitter_dictionary:\n",
    "#                     twitter_dictionary[lower_case] = word_index\n",
    "#                     r = [lower_case, word_index]\n",
    "#                     dic_writer.writerow(r)\n",
    "\n",
    "                row = [token.text, token.lemma_, lower_case, word_index, token.pos_, token.dep_, token.is_stop]\n",
    "\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Keywords using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Activation, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow_addons as tfa \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "hidden_size = 100\n",
    "word_embedding_dim = 300\n",
    "epochs = 10\n",
    "use_unlabeled_dataset = True\n",
    "\n",
    "labeled_dataset_size = 1830\n",
    "train_dataset_size = 900\n",
    "validation_dataset_size = 100\n",
    "test_dataset_size = 830\n",
    "unlabeled_dataset_size = 4000\n",
    "\n",
    "pos_list = np.char.lower([\"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"VERB\",\"X\"])\n",
    "dep_list = np.char.lower([\"ROOT\", \"acl\", \"acomp\", \"advcl\", \"advmod\", \"agent\", \"amod\", \"appos\", \"attr\", \"aux\", \"auxpass\", \"case\", \"cc\", \"ccomp\", \"compound\", \"conj\", \"csubj\", \"csubjpass\", \"dative\", \"dep\", \"det\", \"dobj\", \"expl\", \"intj\", \"mark\", \"meta\", \"neg\", \"nmod\", \"npadvmod\", \"nsubj\", \"nsubjpass\", \"nummod\", \"oprd\", \"parataxis\", \"pcomp\", \"pobj\", \"poss\", \"preconj\", \"predet\", \"prep\", \"prt\", \"punct\", \"quantmod\", \"relcl\", \"xcomp\"])\n",
    "\n",
    "pos_dim = len(pos_list)\n",
    "dep_dim = len(dep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to one-hot encode the labels\n",
    "def one_hot(vec, dic):\n",
    "    vec = np.char.lower(vec)\n",
    "    return np.array([dic == row for row in vec], dtype='i1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(filename):\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        tweet = [tuple(x) for x in spamreader]\n",
    "        tweet = np.array(tweet, dtype=([(\"text\", 'U20'),(\"simplified_text\", 'U20'), (\"best_match\", 'U20'), (\"index\", int), (\"pos\", 'U20'), (\"dep\", 'U20'), (\"stop\", 'U5')]))\n",
    "        \n",
    "    text = tf.reshape(tweet[\"index\"], (1, -1, 1))\n",
    "    pos = tf.reshape(one_hot(tweet[\"pos\"], pos_list), (1, -1, pos_dim))\n",
    "    dep = tf.reshape(one_hot(tweet[\"dep\"], dep_list), (1, -1, dep_dim))\n",
    "    data = np.concatenate((text, pos, dep), axis=-1)\n",
    "    return data, tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f70657384c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(None, pos_dim+dep_dim+1))\n",
    "x = Embedding(380000, word_embedding_dim)(inputs[:,:,0])\n",
    "x = Concatenate(axis=-1)([inputs[:,:,1:], x])\n",
    "x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "outputs = Dense(2, activation=tf.nn.sigmoid)(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# model.summary()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False\n",
    ")\n",
    "model.compile(loss=BinaryCrossentropy(), optimizer=opt)\n",
    "model.load_weights('./checkpoint/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, data, tweet):\n",
    "#     print(\"tweet\", tweet)\n",
    "    prediction = model.predict(data)\n",
    "    keywords = set()\n",
    "#     print(\"prediction\", prediction)\n",
    "    \n",
    "    for i in range(prediction[0].shape[0]):\n",
    "        if (prediction[0][i][1] > 0.32):\n",
    "            keywords.add(tweet[i][0])\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch GIFs using GIPHY API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import giphy_client\n",
    "from giphy_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "from IPython.display import HTML\n",
    "\n",
    "def generate_gif(keywords):\n",
    "    # create an instance of the API class\n",
    "    api_instance = giphy_client.DefaultApi()\n",
    "    api_key = 'dc6zaTOxFJmzC' # str | Giphy API Key.\n",
    "    limit = 2 # int | The maximum number of records to return. (optional) (default to 25)\n",
    "    offset = 0 # int | An optional results offset. Defaults to 0. (optional) (default to 0)\n",
    "    rating = 'g' # str | Filters results by specified rating. (optional)\n",
    "    lang = 'en' # str | Specify default country for regional content; use a 2-letter ISO 639-1 country code. See list of supported languages <a href = \\\"../language-support\\\">here</a>. (optional)\n",
    "    fmt = 'json' # str | Used to indicate the expected response format. Default is Json. (optional) (default to json)\n",
    "\n",
    "    gif = \"\"\n",
    "\n",
    "    for k in keywords:\n",
    "        q = k # str | Search query term or prhase.\n",
    "\n",
    "        # Search Endpoint\n",
    "        api_response = api_instance.gifs_search_get(api_key, q, limit=limit, offset=offset, rating=rating, lang=lang, fmt=fmt)\n",
    "        urls = []\n",
    "        for i in range(len(api_response.data)):\n",
    "            urls.append(api_response.data[i].images.downsized.url)\n",
    "\n",
    "        for i in range(len(urls)):\n",
    "            gif += \"<img src='\" + urls[i] + \"'>\"\n",
    "    \n",
    "    return gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your tweet: I love hot pot\n",
      "The entered tweet is: I love hot pot \n",
      "\n",
      "The predicted keywords are {'pot'} \n",
      "\n",
      "Recommended GIFs are ... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src='https://media4.giphy.com/media/3o6fJ3Zec1OLoSKnF6/giphy.gif?cid=e1bb72ffoq62793yk98ilvm28mkzeqfxxzwcvzlgg3bkhtpg&rid=giphy.gif'><img src='https://media1.giphy.com/media/3o752gfNsKH5y0SXhC/giphy-downsized.gif?cid=e1bb72ffoq62793yk98ilvm28mkzeqfxxzwcvzlgg3bkhtpg&rid=giphy-downsized.gif'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = input(\"Enter your tweet: \")\n",
    "preprocess_tweet(tweet, \"user_input.csv\")\n",
    "print(\"The entered tweet is:\", tweet, \"\\n\")\n",
    "\n",
    "data, tweet = reshape_input(\"user_input.csv\")\n",
    "\n",
    "keywords = make_prediction(model, data, tweet)\n",
    "print(\"The predicted keywords are\", keywords, \"\\n\")\n",
    "\n",
    "gif = generate_gif(keywords)\n",
    "print(\"Recommended GIFs are ... \")\n",
    "HTML(gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"user_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
